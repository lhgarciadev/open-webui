# Fase 1.1 - Ejecutar CreaciÃ³n de HF Space

## Objetivo
Crear un Hugging Face Space con Dockerfile para ejecutar Ollama con modelos LLM.

## Contexto
- Space tipo Docker permite contenedor personalizado
- Puerto 7860 es el estÃ¡ndar de HF Spaces
- ZeroGPU se configura despuÃ©s del build inicial

## Prompt de EjecuciÃ³n

```
Crea un Hugging Face Space para Ollama:

1. CREAR DIRECTORIO LOCAL:

   # Crear carpeta para el Space
   mkdir -p ~/cognitia-ollama-space
   cd ~/cognitia-ollama-space

2. CREAR DOCKERFILE:

   cat > Dockerfile << 'EOF'
FROM ollama/ollama:latest

# ConfiguraciÃ³n para HF Spaces + ZeroGPU
ENV OLLAMA_HOST=0.0.0.0
ENV OLLAMA_KEEP_ALIVE=24h
ENV OLLAMA_NUM_PARALLEL=4
ENV OLLAMA_FLASH_ATTENTION=1

# Puerto estÃ¡ndar HF Spaces
ENV PORT=7860

# Directorio de trabajo
WORKDIR /app

# Copiar script de inicio
COPY start.sh /app/start.sh
RUN chmod +x /app/start.sh

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:7860/api/tags || exit 1

# Puerto
EXPOSE 7860

# Iniciar
CMD ["/app/start.sh"]
EOF

3. CREAR SCRIPT DE INICIO:

   cat > start.sh << 'EOF'
#!/bin/bash
set -e

echo "========================================="
echo "  Cognitia Ollama - Hugging Face Space"
echo "========================================="

# Puerto para HF Spaces
export OLLAMA_HOST=0.0.0.0:7860

echo "[1/4] Iniciando servidor Ollama..."
ollama serve &
OLLAMA_PID=$!

echo "[2/4] Esperando que Ollama estÃ© listo..."
MAX_ATTEMPTS=60
ATTEMPT=0
until curl -s http://localhost:7860/api/tags > /dev/null 2>&1; do
    ATTEMPT=$((ATTEMPT + 1))
    if [ $ATTEMPT -ge $MAX_ATTEMPTS ]; then
        echo "ERROR: Ollama no iniciÃ³ despuÃ©s de ${MAX_ATTEMPTS} intentos"
        exit 1
    fi
    echo "  Esperando... intento $ATTEMPT/$MAX_ATTEMPTS"
    sleep 2
done
echo "  Ollama estÃ¡ listo!"

echo "[3/4] Descargando modelos..."

# Qwen2.5 7B - Mejor balance calidad/eficiencia
if ! ollama list | grep -q "qwen2.5:7b"; then
    echo "  Descargando qwen2.5:7b (~4.7GB)..."
    ollama pull qwen2.5:7b
else
    echo "  qwen2.5:7b ya disponible"
fi

# Phi3 - RÃ¡pido, bajo consumo de cuota
if ! ollama list | grep -q "phi3"; then
    echo "  Descargando phi3 (~2.3GB)..."
    ollama pull phi3
else
    echo "  phi3 ya disponible"
fi

# CodeLlama 7B - Para programaciÃ³n
if ! ollama list | grep -q "codellama:7b"; then
    echo "  Descargando codellama:7b (~3.8GB)..."
    ollama pull codellama:7b
else
    echo "  codellama:7b ya disponible"
fi

echo "[4/4] Modelos disponibles:"
ollama list

echo ""
echo "========================================="
echo "  Ollama listo en puerto 7860"
echo "  Modelos: qwen2.5:7b, phi3, codellama:7b"
echo "========================================="

# Mantener proceso activo
wait $OLLAMA_PID
EOF

   chmod +x start.sh

4. CREAR README.md (METADATA HF):

   cat > README.md << 'EOF'
---
title: Cognitia Ollama
emoji: ðŸ¦™
colorFrom: blue
colorTo: indigo
sdk: docker
app_port: 7860
suggested_hardware: zero-a10g
pinned: false
license: mit
---

# Cognitia Ollama Service

Servicio Ollama con modelos LLM opensource para Cognitia.

## Modelos Disponibles

| Modelo | ParÃ¡metros | Uso |
|--------|------------|-----|
| qwen2.5:7b | 7B | Chat general, coding |
| phi3 | 3.8B | Respuestas rÃ¡pidas |
| codellama:7b | 7B | ProgramaciÃ³n |

## API Endpoints

```bash
# Listar modelos
curl https://[space-url]/api/tags

# Generar respuesta
curl https://[space-url]/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:7b", "prompt": "Hola"}'

# Chat
curl https://[space-url]/api/chat \
  -H "Content-Type: application/json" \
  -d '{"model": "phi3", "messages": [{"role": "user", "content": "Hola"}]}'
```

## Hardware

Este Space usa ZeroGPU (NVIDIA H200) para inferencia acelerada.
EOF

5. CREAR REPOSITORIO EN HF:

   # Crear Space (reemplaza TU_USUARIO)
   huggingface-cli repo create cognitia-ollama --type space --space-sdk docker -y

6. INICIALIZAR GIT Y PUSH:

   # Inicializar git
   git init
   git add .
   git commit -m "feat: Initial Ollama setup for HF Spaces"

   # Agregar remote (reemplaza TU_USUARIO)
   git remote add origin https://huggingface.co/spaces/TU_USUARIO/cognitia-ollama

   # Push
   git push -u origin main

7. VERIFICAR BUILD:

   # Abrir en navegador (reemplaza TU_USUARIO)
   echo "Abrir: https://huggingface.co/spaces/TU_USUARIO/cognitia-ollama"

   # El build tomarÃ¡ 2-5 minutos
   # Ver logs en la pestaÃ±a "Logs" del Space

Entrega:
- URL del Space creado
- Screenshot del build en progreso
- ConfirmaciÃ³n de archivos subidos
```

## Estructura de Archivos

```
cognitia-ollama-space/
â”œâ”€â”€ Dockerfile      # Contenedor Ollama
â”œâ”€â”€ start.sh        # Script de inicio + descarga modelos
â””â”€â”€ README.md       # Metadata del Space
```

## Checklist de EjecuciÃ³n

```
ARCHIVOS
[ ] Dockerfile creado
[ ] start.sh creado y ejecutable
[ ] README.md con metadata YAML

REPOSITORIO
[ ] Space creado en HF
[ ] Git inicializado
[ ] Remote configurado
[ ] Push exitoso

BUILD
[ ] Build iniciado en HF
[ ] Sin errores en logs
[ ] Status: Building o Running
```

## Troubleshooting

### Push rechazado
```bash
# Verificar remote
git remote -v

# Si estÃ¡ mal, corregir
git remote set-url origin https://huggingface.co/spaces/TU_USUARIO/cognitia-ollama

# Forzar push si es necesario
git push -u origin main --force
```

### Build falla
```bash
# Ver logs en HF
# https://huggingface.co/spaces/TU_USUARIO/cognitia-ollama

# Errores comunes:
# - Puerto incorrecto â†’ Verificar EXPOSE 7860
# - Script no ejecutable â†’ chmod +x start.sh
# - Sintaxis YAML â†’ Verificar README.md
```

## Tiempo Estimado

| Paso | Tiempo |
|------|--------|
| Crear archivos | 3-5 min |
| Crear repo HF | 1 min |
| Push | 1-2 min |
| Build | 3-5 min |
| **Total** | **8-13 min** |

## Siguiente Paso

Continuar a [1.2-space-validar.md](./1.2-space-validar.md) para validar el Space.
