# Fase 5.1 - Ejecutar Optimizaci√≥n

## Objetivo

Optimizar el setup agregando modelos adicionales y ajustando configuraciones de performance.

## Contexto

- La migraci√≥n b√°sica est√° completa
- Esta fase es opcional pero recomendada
- Agrega m√°s modelos y mejora la experiencia

## Prompt de Ejecuci√≥n

````
Optimiza el Space de Ollama con modelos adicionales y configuraciones:

1. EVALUAR USO DE CUOTA:

   # Verificar cuota actual
   # https://huggingface.co/settings/billing

   # Si cuota > 50%: proceder con modelos adicionales
   # Si cuota < 30%: considerar PRO o limitar modelos

2. AGREGAR MODELOS ADICIONALES (Opcional):

   # Editar start.sh local
   cd ~/cognitia-ollama-space

   # Agregar al final de la secci√≥n de modelos:
   cat >> start.sh << 'EOF'

# Modelos adicionales (Fase 5 - Optimizaci√≥n)

# Gemma 2 9B - Excelente para razonamiento
if ! ollama list | grep -q "gemma2:9b"; then
    echo "  Descargando gemma2:9b (~5.4GB)..."
    ollama pull gemma2:9b
else
    echo "  gemma2:9b ya disponible"
fi

# Llama 3.2 3B - Ultra r√°pido para respuestas simples
if ! ollama list | grep -q "llama3.2:3b"; then
    echo "  Descargando llama3.2:3b (~2GB)..."
    ollama pull llama3.2:3b
else
    echo "  llama3.2:3b ya disponible"
fi

# Mistral 7B - Vers√°til y popular
if ! ollama list | grep -q "mistral:7b"; then
    echo "  Descargando mistral:7b (~4.1GB)..."
    ollama pull mistral:7b
else
    echo "  mistral:7b ya disponible"
fi
EOF

3. OPTIMIZAR DOCKERFILE:

   # Agregar variables de optimizaci√≥n
   cat > Dockerfile << 'EOF'
FROM ollama/ollama:latest

# Optimizaciones de GPU
ENV OLLAMA_HOST=0.0.0.0
ENV OLLAMA_KEEP_ALIVE=24h
ENV OLLAMA_NUM_PARALLEL=4
ENV OLLAMA_FLASH_ATTENTION=1
ENV OLLAMA_MAX_LOADED_MODELS=2
ENV OLLAMA_NUM_GPU=999

# Puerto HF Spaces
ENV PORT=7860

WORKDIR /app
COPY start.sh /app/start.sh
RUN chmod +x /app/start.sh

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:7860/api/tags || exit 1

EXPOSE 7860
CMD ["/app/start.sh"]
EOF

4. ACTUALIZAR README CON NUEVOS MODELOS:

   cat > README.md << 'EOF'
---
title: Cognitia Ollama
emoji: ü¶ô
colorFrom: blue
colorTo: indigo
sdk: docker
app_port: 7860
suggested_hardware: zero-a10g
pinned: false
license: mit
---

# Cognitia Ollama Service

Servicio Ollama optimizado con modelos LLM opensource.

## Modelos Disponibles

### Tier 1 - Principales
| Modelo | Par√°metros | Uso | Velocidad |
|--------|------------|-----|-----------|
| qwen2.5:7b | 7B | Chat + Coding | ‚≠ê‚≠ê‚≠ê‚≠ê |
| phi3 | 3.8B | Respuestas r√°pidas | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| codellama:7b | 7B | Programaci√≥n | ‚≠ê‚≠ê‚≠ê‚≠ê |

### Tier 2 - Adicionales
| Modelo | Par√°metros | Uso | Velocidad |
|--------|------------|-----|-----------|
| gemma2:9b | 9B | Razonamiento | ‚≠ê‚≠ê‚≠ê |
| llama3.2:3b | 3B | Chat ligero | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| mistral:7b | 7B | Versatilidad | ‚≠ê‚≠ê‚≠ê‚≠ê |

## Hardware

- GPU: ZeroGPU (NVIDIA H200)
- VRAM: ~70GB compartida
- Optimizaciones: Flash Attention, Multi-GPU

## API

```bash
curl https://[space-url]/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:7b", "prompt": "Hola"}'
````

EOF

5. PUSH CAMBIOS:

   git add .
   git commit -m "feat: Add additional models and GPU optimizations"
   git push

   # El Space se reconstruir√°

   # Tiempo estimado: 5-10 min + descarga de modelos

6. CONSIDERAR PRO (si cuota insuficiente):

   # Si la cuota se agota frecuentemente:

   # https://huggingface.co/pricing

   # PRO = $9/mes ‚Üí 8x m√°s cuota + prioridad

7. CONFIGURAR ALIASES EN COGNITIA (Opcional):

   # En Cognitia Settings > Models

   # Crear aliases amigables:

   # - "Chat R√°pido" ‚Üí phi3

   # - "Chat Inteligente" ‚Üí qwen2.5:7b

   # - "Programaci√≥n" ‚Üí codellama:7b

Entrega:

- Lista de modelos actualizada
- Screenshot de modelos en Cognitia
- M√©tricas de cuota

```

## Modelos Adicionales Recomendados

| Modelo | Tama√±o | Cuota | Recomendaci√≥n |
|--------|--------|-------|---------------|
| gemma2:9b | 5.4GB | Media | ‚≠ê Razonamiento |
| llama3.2:3b | 2GB | Baja | ‚≠ê‚≠ê Velocidad |
| mistral:7b | 4.1GB | Media | ‚≠ê Versatilidad |

## Checklist de Ejecuci√≥n

```

MODELOS ADICIONALES
[ ] start.sh actualizado
[ ] Nuevos modelos agregados
[ ] Push realizado
[ ] Build exitoso

OPTIMIZACIONES
[ ] Dockerfile optimizado
[ ] Variables de GPU configuradas
[ ] Flash Attention habilitado

DOCUMENTACI√ìN
[ ] README actualizado
[ ] Lista de modelos correcta

```

## Tiempo Estimado

| Paso | Tiempo |
|------|--------|
| Editar archivos | 5-10 min |
| Push y build | 5-10 min |
| Descarga modelos | 20-40 min |
| **Total** | **30-60 min** |

## Siguiente Paso

Continuar a [5.2-optimize-validar.md](./5.2-optimize-validar.md) para validar optimizaciones.
```
