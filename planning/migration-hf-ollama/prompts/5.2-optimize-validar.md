# Fase 5.2 - Validar Optimización

## Objetivo
Verificar que las optimizaciones y modelos adicionales funcionan correctamente.

## Prompt de Validación

```
Valida las optimizaciones del Space de Ollama:

1. VERIFICAR MODELOS TOTALES:

   # Listar todos los modelos (reemplaza TU_USUARIO)
   curl -s https://TU_USUARIO-cognitia-ollama.hf.space/api/tags | jq '.models[].name'

   # Esperado (6 modelos):
   # - qwen2.5:7b
   # - phi3
   # - codellama:7b
   # - gemma2:9b (nuevo)
   # - llama3.2:3b (nuevo)
   # - mistral:7b (nuevo)

2. TEST DE MODELOS NUEVOS:

   SPACE_URL="https://TU_USUARIO-cognitia-ollama.hf.space"

   # Test Gemma 2 (razonamiento)
   echo "=== Gemma 2 9B ==="
   time curl -s "$SPACE_URL/api/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "gemma2:9b",
       "prompt": "Explica en una oración por qué el cielo es azul",
       "stream": false
     }' | jq -r '.response'

   # Test Llama 3.2 (velocidad)
   echo "=== Llama 3.2 3B ==="
   time curl -s "$SPACE_URL/api/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "llama3.2:3b",
       "prompt": "Hola",
       "stream": false
     }' | jq -r '.response'

   # Test Mistral (versatilidad)
   echo "=== Mistral 7B ==="
   time curl -s "$SPACE_URL/api/generate" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "mistral:7b",
       "prompt": "Write a haiku about programming",
       "stream": false
     }' | jq -r '.response'

3. BENCHMARK COMPARATIVO:

   echo "=== BENCHMARK COMPARATIVO ==="
   for model in "phi3" "llama3.2:3b" "qwen2.5:7b" "mistral:7b" "gemma2:9b"; do
     echo ""
     echo "Modelo: $model"
     start=$(date +%s.%N)
     curl -s "$SPACE_URL/api/generate" \
       -H "Content-Type: application/json" \
       -d "{\"model\": \"$model\", \"prompt\": \"Count to 5\", \"stream\": false}" > /dev/null
     end=$(date +%s.%N)
     elapsed=$(echo "$end - $start" | bc)
     echo "  Latencia: ${elapsed}s"
   done

4. VERIFICAR EN COGNITIA:

   a) Abrir https://cognitia-production.up.railway.app
   b) Click en selector de modelos
   c) Verificar que aparecen los 6 modelos
   d) Probar cada modelo nuevo con un chat

5. VERIFICAR CUOTA:

   # https://huggingface.co/settings/billing
   # Verificar uso de cuota después de tests
   # Si uso > 80%: considerar PRO

6. VERIFICAR OPTIMIZACIONES GPU:

   # En logs del Space, buscar:
   # - "Flash Attention enabled"
   # - "Using GPU"
   # - Sin errores de memoria

7. CREAR REPORTE FINAL:

   echo "=== REPORTE DE OPTIMIZACIÓN ==="
   echo "Modelos totales: $(curl -s $SPACE_URL/api/tags | jq '.models | length')"
   echo "Modelos principales: qwen2.5:7b, phi3, codellama:7b"
   echo "Modelos adicionales: gemma2:9b, llama3.2:3b, mistral:7b"
   echo "Hardware: ZeroGPU (H200)"
   echo "Optimizaciones: Flash Attention, Multi-GPU"

Resultado esperado:
- 6 modelos disponibles
- Todos responden correctamente
- Latencia < 3s para todos
- Cuota suficiente
```

## Checklist de Validación

```
MODELOS
[ ] 6 modelos en total
[ ] Modelos originales funcionan (3)
[ ] Modelos nuevos funcionan (3)

PERFORMANCE
[ ] phi3 < 1.5s
[ ] llama3.2:3b < 1.5s
[ ] qwen2.5:7b < 2.5s
[ ] mistral:7b < 2.5s
[ ] gemma2:9b < 3s

COGNITIA
[ ] Todos los modelos visibles
[ ] Chat funciona con cada modelo
[ ] Sin errores de conexión

CUOTA
[ ] Uso de cuota monitoreado
[ ] Suficiente para uso diario
```

## Criterios de Éxito

| Criterio | Esperado | Crítico |
|----------|----------|---------|
| Total modelos | 6 | No |
| Modelos funcionan | Todos | Sí |
| Latencia máxima | < 3s | Sí |
| Cuota disponible | > 20% | Sí |

## Ranking de Modelos por Caso de Uso

| Caso de Uso | Mejor Modelo | Alternativa |
|-------------|--------------|-------------|
| Chat rápido | llama3.2:3b | phi3 |
| Chat inteligente | qwen2.5:7b | gemma2:9b |
| Código | codellama:7b | qwen2.5:7b |
| Razonamiento | gemma2:9b | qwen2.5:7b |
| Versatilidad | mistral:7b | qwen2.5:7b |

## Si Falla

### Modelo no descargó
```bash
# Ver logs del Space
# Buscar errores de descarga

# Si hay error de espacio:
# Reducir número de modelos
# Eliminar modelos menos usados
```

### Latencia alta en modelo grande
```bash
# Modelos grandes (9B+) son más lentos
# Esto es normal
# Usar modelos más pequeños para velocidad
```

### Cuota agotada
```bash
# Opciones:
# 1. Reducir uso (menos tests)
# 2. Upgrade a PRO ($9/mes)
# 3. Esperar reset diario
```

## Siguiente Paso

Si todos los criterios pasan: Continuar a [6.1-cleanup-ejecutar.md](./6.1-cleanup-ejecutar.md)

Si falta algún modelo: Verificar y corregir.
