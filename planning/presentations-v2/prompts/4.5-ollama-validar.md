# Etapa 4.5 - Validar Ollama + Phi3 (Seguridad)

## Objetivo
Verificar que el entorno local funciona correctamente y que los datos no salen de la red.

## Prompt de Validacion

```
Valida el despliegue de Ollama y las pruebas de seguridad:

1. VERIFICAR SERVICIO ACTIVO:

   # Ollama corriendo
   pgrep -l ollama
   # Deberia mostrar: XXXX ollama

   # API respondiendo
   curl -s http://localhost:11434/api/tags | jq '.models[].name'
   # Deberia mostrar: phi3:latest

2. VERIFICAR MODELO FUNCIONAL:

   # Test basico
   ollama run phi3 "Responde solo 'OK' si funcionas"

   # Test en espanol
   ollama run phi3 "Cuenta del 1 al 5 en espanol"

3. TEST DE SEGURIDAD - TRAFICO DE RED:

   # En terminal 1: Monitorear trafico
   # macOS:
   sudo tcpdump -i any port 443 or port 80 -c 50

   # Linux:
   sudo tcpdump -i any port 443 or port 80 -c 50

   # En terminal 2: Usar el chat
   # - Abrir Open WebUI
   # - Seleccionar phi3
   # - Enviar varios mensajes

   # Verificar en terminal 1:
   # NO deberia haber trafico a IPs externas
   # Solo trafico local (127.0.0.1, localhost)

4. TEST DE SEGURIDAD - FUNCIONAMIENTO OFFLINE:

   # Desconectar internet temporalmente
   # macOS:
   networksetup -setairportpower en0 off

   # O desconectar cable/WiFi manualmente

   # Verificar que Ollama sigue funcionando:
   ollama run phi3 "Estoy funcionando offline?"

   # Verificar que Open WebUI + phi3 funciona:
   # - Abrir chat existente
   # - Enviar mensaje
   # - Verificar respuesta

   # Reconectar internet:
   # macOS:
   networksetup -setairportpower en0 on

5. TEST DE AISLAMIENTO DE DATOS:

   # Verificar donde se guardan los datos
   ls -la ~/.ollama/

   # Verificar que los modelos estan locales
   du -sh ~/.ollama/models/

   # Los datos NO deben estar en:
   # - /tmp con nombres sospechosos
   # - Directorios de cloud sync (Dropbox, iCloud, etc)

6. TEST DE INTEGRACION - PRESENTATIONS TOOL:

   # En Open WebUI con phi3 seleccionado:

   Mensaje: "Genera una presentacion de 3 slides sobre ciberseguridad"

   Verificar:
   [ ] La tool se invoca correctamente
   [ ] No hay timeout
   [ ] Presentacion se genera
   [ ] Link de descarga funciona

7. TEST DE RENDIMIENTO:

   # Medir tiempo de respuesta
   time ollama run phi3 "Explica que es machine learning en 2 oraciones"

   # Objetivo: < 30 segundos para respuesta completa

   # Monitorear recursos durante uso:
   # Terminal separada:
   htop  # o Activity Monitor en Mac

8. TEST DE PERSISTENCIA:

   # Reiniciar Ollama
   pkill ollama
   sleep 2
   ollama serve &
   sleep 5

   # Verificar que el modelo sigue disponible
   ollama list

   # Verificar que funciona
   ollama run phi3 "Test post-restart"

Entrega:
- Resultado de cada test (PASS/FAIL)
- Screenshot de tcpdump (sin trafico externo)
- Tiempo de respuesta medido
- Confirmacion de funcionamiento offline
```

## Matriz de Validacion de Seguridad

| Test | Descripcion | Criterio | PASS/FAIL |
|------|-------------|----------|-----------|
| Trafico red | Sin conexiones externas | tcpdump limpio | |
| Offline | Funciona sin internet | Respuestas OK | |
| Datos locales | Todo en ~/.ollama | ls confirma | |
| Sin telemetria | No hay beacons | tcpdump limpio | |

## Matriz de Validacion Funcional

| Test | Descripcion | Criterio | PASS/FAIL |
|------|-------------|----------|-----------|
| API responde | curl exitoso | JSON valido | |
| Modelo carga | phi3 disponible | ollama list | |
| Chat funciona | Respuestas coherentes | UI OK | |
| Tools funcionan | Presentations genera | HTML creado | |
| Rendimiento | Respuesta < 30s | time OK | |
| Persistencia | Sobrevive restart | Test OK | |

## Resultados Esperados

### tcpdump durante uso (CORRECTO - sin trafico externo):
```
# Solo deberia mostrar trafico local como:
# listening on any, link-type LINUX_SLL2
# (sin paquetes capturados a IPs externas)
```

### tcpdump si hay problema (INCORRECTO):
```
# Si ves algo como:
# 192.168.1.X > 52.84.X.X (AWS)
# 192.168.1.X > 104.18.X.X (Cloudflare)
# HAY UN PROBLEMA - datos saliendo de la red
```

## Checklist Final

```
SEGURIDAD
[ ] Sin trafico a IPs externas durante uso
[ ] Funciona completamente offline
[ ] Datos solo en ~/.ollama (local)
[ ] Sin conexiones sospechosas

FUNCIONALIDAD
[ ] Ollama API responde
[ ] phi3 cargado y funcional
[ ] Open WebUI detecta modelo
[ ] Chat funciona correctamente
[ ] Presentations tool funciona
[ ] Rendimiento aceptable (< 30s)

ESTABILIDAD
[ ] Sobrevive restart de Ollama
[ ] Sin memory leaks evidentes
[ ] Sin crashes durante uso
```

## Resultado Final

### Si todo pasa:
```
SEGURIDAD:      ✅ PASS - Datos permanecen locales
OFFLINE:        ✅ PASS - Funciona sin internet
FUNCIONALIDAD:  ✅ PASS - Todas las features OK
RENDIMIENTO:    ✅ PASS - Respuestas < 30s

OLLAMA + PHI3:  ✅ LISTO PARA PRUEBAS
```

### Si hay issues:
```
[TEST]:         ❌ FAIL - [descripcion]
ACCION:         [pasos para corregir]
```

## Siguiente Paso

Si pasa -> Continuar a 4.6-upstream-sync-ejecutar.md
